{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05: Training a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://github.com/guiwitz/DLImaging/raw/master/illustrations/ML_principle.jpg\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# set path containing data folder or use default for Colab (/gdrive/My Drive)\n",
    "local_folder = \"../data/\"\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/guiwitz/DLImaging/master/utils/check_colab.py', 'check_colab.py')\n",
    "from check_colab import set_datapath\n",
    "colab, datapath = set_datapath(local_folder)\n",
    "\n",
    "Image(url='https://github.com/guiwitz/DLImaging/raw/master/illustrations/ML_principle.jpg',width=700)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "   1. First we will pass training examples forward through the network\n",
    "   2. We measure an error between prediction and true label, the loss\n",
    "   3. We calculate the gradient of the loss respective to each parameter in the model. This is done by backpropagation\n",
    "   4. We adjust the parameters using the calculated gradient and an optimizer (e.g. SGD)\n",
    "\n",
    "Additionally we will also see in this notebooks additional aspects such as training epochs and validation. The goal here is to once see the whole pipeline in detail before we start using tools that reduce some of the boiler-plate code necessary here.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create our network and define a loss, let's remember how training samples are passed through the network. In principle we want to do each optimization step for the entire dataset not just a single image as training would have a difficult time to converge. However this is usually not possible and and instead what is generally done is to use mini-batches, i.e. the network is iteratively trained on subsets of traininig examples. So now instead of using the gradients produced by a single image, one can use for example the average gradients over the mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://github.com/guiwitz/DLImaging/raw/master/illustrations/batch_processing.jpg\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://github.com/guiwitz/DLImaging/raw/master/illustrations/batch_processing.jpg',width=700) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "PyTorch is in fact designed to handle batches by default. We can see that if we look at the documentation of modules such as nn.Linear which says that inputs should have the shape N x ... where N stands for batch size and ... for other dimensions such as channels, samples etc. This applies in fact to all modules, including those calculating losses. We can therefore feed examples with dimensions N x ... and PyTorch handles batch calculations for us.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this mean for out network? We only have to make one slight modification. We used x.view(-1) previously to flatten 32x32 images into vectors of 1024 elements. If we now feed a batch of size Nx32x32, this would generate a long vector of size Nx1024. So we need to adjust the view() command and specify the size of the first dimension. In such a way only the image dimensions are flattened: x.view(batchsize, -1). Alternatively we can use torch.flatten(start_dim = 1) specifying from which dimension we want to start flattening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mynetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_categories):\n",
    "        super(Mynetwork, self).__init__()\n",
    "\n",
    "        # define layers:\n",
    "        self.layer1 = nn.Linear(input_size, 100)\n",
    "        self.layer2 = nn.Linear(100, 100)\n",
    "        self.layer3 = nn.Linear(100, num_categories)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #flatten the input\n",
    "        x = x.flatten(start_dim=1)\n",
    "        # define the sequence of operations in the network including activations for example\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insantiate \n",
    "model = Mynetwork(1024, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check inputs/outputs\n",
    "\n",
    "myinput = torch.randn((5,32,32))\n",
    "myinput.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myoutput = model(myinput)\n",
    "myoutput.size()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If we want to pass a single element, e.g. in the inference phase, then we still have to reshape it so that it has dimensions N x .... The first dimension will just have a size of 1. The simple way to do that is to use unsqueeze():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myimage = torch.randn((32,32))\n",
    "myimage.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myimage = myimage.unsqueeze(0)\n",
    "myimage.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(myimage)\n",
    "output.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a loss function and backpropogation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example below - classify images. Therefore, use a standard loss function like cross entropy avaiable in torch.nn module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.loss.CrossEntropyLoss"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(criterion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss function is a module: it is differentiable and can be integrated into the network. It sticks to the same \"batch-logic\" as the other layers. Therefore it expects inputs where dims start with N for batches. \n",
    "- So need an output of the network size: N x C, where C is the number of categories (eg. 2 in this example) and a list of target labels (\"true\" labels) which have to be turned into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some data:\n",
    "mysample = torch.randn(3, 32*32)\n",
    "mylabels = torch.tensor([0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass them through the network\n",
    "output = model(mysample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare output to target with cross-entropy module:\n",
    "loss = criterion(output, mylabels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: CrossEntropyLoss module automatically applies soft-mas to the output and then calculates loss. We DONT NEED to have a soft-max layer at the end of network.\n",
    "\n",
    "After the forward pass, we can calculate the gradients of loss by backpropogation. Done by calling \"backward\" method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have estimates of loss and the gradients, so can now optimize all our parameters bt using optimisation algorithms. Several are available to use in torch.optim. Here using Adam optimiser, of the \"safer\" choices. As arguments we need to pass a list of parameters that need to be optimized. We can do that by recovering them from our model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0169, -0.0028,  0.0166,  ...,  0.0138, -0.0294,  0.0260],\n",
       "         [ 0.0031, -0.0187, -0.0098,  ...,  0.0274, -0.0208, -0.0231],\n",
       "         [ 0.0205,  0.0225,  0.0139,  ...,  0.0033, -0.0183, -0.0188],\n",
       "         ...,\n",
       "         [-0.0089, -0.0237, -0.0096,  ...,  0.0250,  0.0010, -0.0275],\n",
       "         [ 0.0103, -0.0147,  0.0073,  ..., -0.0162, -0.0283,  0.0303],\n",
       "         [ 0.0106, -0.0227,  0.0006,  ...,  0.0209,  0.0262,  0.0079]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0217, -0.0023,  0.0276,  0.0068,  0.0037, -0.0254,  0.0056, -0.0081,\n",
       "         -0.0129, -0.0228,  0.0088,  0.0299,  0.0299,  0.0138,  0.0310,  0.0145,\n",
       "          0.0083,  0.0136, -0.0153, -0.0108,  0.0017, -0.0076, -0.0203, -0.0193,\n",
       "          0.0071, -0.0081, -0.0294, -0.0224,  0.0198,  0.0311, -0.0055, -0.0011,\n",
       "         -0.0282, -0.0014, -0.0003, -0.0029,  0.0167,  0.0261,  0.0087, -0.0245,\n",
       "         -0.0095, -0.0286,  0.0157, -0.0033, -0.0280, -0.0123, -0.0305,  0.0054,\n",
       "         -0.0080, -0.0138,  0.0218, -0.0121, -0.0177,  0.0285, -0.0167,  0.0182,\n",
       "         -0.0031, -0.0204, -0.0274,  0.0309,  0.0127,  0.0115, -0.0029,  0.0180,\n",
       "         -0.0202, -0.0004, -0.0215, -0.0023,  0.0235,  0.0006,  0.0021,  0.0210,\n",
       "          0.0105, -0.0033,  0.0162,  0.0300, -0.0034,  0.0254, -0.0168, -0.0086,\n",
       "         -0.0202, -0.0103, -0.0157, -0.0102, -0.0029,  0.0271,  0.0210,  0.0097,\n",
       "          0.0022, -0.0150, -0.0303, -0.0033,  0.0089,  0.0050, -0.0257, -0.0201,\n",
       "          0.0128,  0.0211,  0.0120,  0.0087], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0991,  0.0853,  0.0970,  ...,  0.0003, -0.0971, -0.0390],\n",
       "         [ 0.0400,  0.0490, -0.0319,  ..., -0.0518,  0.0207, -0.0226],\n",
       "         [-0.0940, -0.0699, -0.0007,  ...,  0.0649, -0.0071,  0.0705],\n",
       "         ...,\n",
       "         [-0.0479, -0.0089, -0.0953,  ...,  0.0412, -0.0980, -0.0606],\n",
       "         [ 0.0351, -0.0999, -0.0426,  ...,  0.0086, -0.0582,  0.0950],\n",
       "         [ 0.0530, -0.0794, -0.0035,  ..., -0.0129, -0.0326, -0.0429]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0260,  0.0416, -0.0078, -0.0752, -0.0064, -0.0705, -0.0108,  0.0297,\n",
       "          0.0088, -0.0817,  0.0779,  0.0263,  0.0411,  0.0323,  0.0210, -0.0166,\n",
       "         -0.0012, -0.0146,  0.0053, -0.0307,  0.0567, -0.0656, -0.0852, -0.0322,\n",
       "         -0.0632, -0.0640, -0.0844, -0.0209,  0.0177,  0.0066, -0.0616, -0.0567,\n",
       "         -0.0891, -0.0623,  0.0083,  0.0076,  0.0180, -0.0385, -0.0107,  0.0436,\n",
       "         -0.0210, -0.0126, -0.0199, -0.0026,  0.0664,  0.0216, -0.0349,  0.0796,\n",
       "          0.0475, -0.0080,  0.0649,  0.0152, -0.0907,  0.0306,  0.0027, -0.0511,\n",
       "         -0.0112, -0.0185, -0.0022,  0.0665, -0.0033,  0.0686,  0.0808, -0.0426,\n",
       "         -0.0913,  0.0103,  0.0859, -0.0216,  0.0213, -0.0333,  0.0617,  0.0495,\n",
       "         -0.0120,  0.0433, -0.0712, -0.0991, -0.0370,  0.0291, -0.0648,  0.0485,\n",
       "          0.0087,  0.0829, -0.0197, -0.0798,  0.0660, -0.0303,  0.0412, -0.0852,\n",
       "          0.0995,  0.0389,  0.0493,  0.0921,  0.0228, -0.0610,  0.0434, -0.0640,\n",
       "          0.0159,  0.0759, -0.0979,  0.0950], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0560, -0.0962, -0.0355,  0.0483, -0.0835, -0.0662, -0.0037,  0.0873,\n",
       "           0.0856,  0.0283,  0.0015,  0.0372, -0.0280,  0.0497, -0.0501, -0.0127,\n",
       "           0.0695,  0.0666,  0.0397, -0.0787, -0.0042, -0.0703, -0.0451,  0.0413,\n",
       "          -0.0102, -0.0512,  0.0954, -0.0800,  0.0995,  0.0697, -0.0755, -0.0086,\n",
       "           0.0390,  0.0642, -0.0863,  0.0178, -0.0922,  0.0371,  0.0544,  0.0564,\n",
       "          -0.0313, -0.0429, -0.0758, -0.0073,  0.0409,  0.0822, -0.0469, -0.0320,\n",
       "           0.0739,  0.0202,  0.0723,  0.0694,  0.0843, -0.0068,  0.0818, -0.0729,\n",
       "           0.0833,  0.0910, -0.0368,  0.0163, -0.0544,  0.0945, -0.0566,  0.0387,\n",
       "          -0.0701, -0.0025,  0.0458,  0.0254, -0.0078, -0.0638, -0.0393, -0.0017,\n",
       "          -0.0508,  0.0856, -0.0580,  0.0609, -0.0586, -0.0989,  0.0238, -0.0255,\n",
       "          -0.0102,  0.0213,  0.0156, -0.0628, -0.0632, -0.0275,  0.0111,  0.0426,\n",
       "           0.0221, -0.0665,  0.0904,  0.0349, -0.0891, -0.0441, -0.0746, -0.0119,\n",
       "           0.0109,  0.0976, -0.0807,  0.0477],\n",
       "         [-0.0255,  0.0298, -0.0399,  0.0142, -0.0869, -0.0838,  0.0445, -0.0188,\n",
       "           0.0713, -0.0787, -0.0322,  0.0932, -0.0230, -0.0719,  0.0290,  0.0909,\n",
       "           0.0507,  0.0472, -0.0407,  0.0287, -0.0992,  0.0875,  0.0709,  0.0141,\n",
       "           0.0905,  0.0365, -0.0025,  0.0138,  0.0813, -0.0922, -0.0653, -0.0320,\n",
       "           0.0925, -0.0724, -0.0209, -0.0383, -0.0950, -0.0212,  0.0006, -0.0720,\n",
       "           0.0198, -0.0789,  0.0735,  0.0699, -0.0231,  0.0979, -0.0225,  0.0290,\n",
       "           0.0464, -0.0490,  0.0577,  0.0400, -0.0831,  0.0843,  0.0944,  0.0208,\n",
       "          -0.0814, -0.0688, -0.0120,  0.0493,  0.0075, -0.0865, -0.0928,  0.0081,\n",
       "           0.0535, -0.0075,  0.0657, -0.0498, -0.0474,  0.0847,  0.0789, -0.0126,\n",
       "           0.0522,  0.0878, -0.0536,  0.0702,  0.0726,  0.0421,  0.0252,  0.0997,\n",
       "           0.0071, -0.0405, -0.0320,  0.0747, -0.0278,  0.0886, -0.0141,  0.0376,\n",
       "           0.0252,  0.0284,  0.0739,  0.0126, -0.0119,  0.0337, -0.0339,  0.0075,\n",
       "           0.0155, -0.0539, -0.0776,  0.0754]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0950, -0.0896], requires_grad=True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use cross-entropy as a loss function because it allows us to optimise the network. However, we are interested in the **accuracy** of the model - whether the correct label has been found or not. A binary answer is not useful for optimisation, but it is the goal to monitor this. Generating random data below to practice calculating this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "myimages = torch.randn((3,32,32))\n",
    "labels = torch.randint(0,2,(3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0885, -0.0422],\n",
       "        [-0.0140, -0.0682],\n",
       "        [-0.0438, -0.0390]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(myimages)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The predicted category is the one with the highest probability (not normalized here but it doesn't matter). We can therefore just look for the index of the maximum value along the horizontal dimension:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare prediction and true label\n",
    "labels == output.argmax(dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take the sum of the tensor (how many samples in the batch were correctly predicted) divided by total samples and the average accuracy is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6667)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(labels == output.argmax(dim=1)).sum()/3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
