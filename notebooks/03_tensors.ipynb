{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: Tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A specific type of array is requried in the context of PyTorch based neural nets, called \"tensors\"\n",
    "- Tensors are similar to NumPy arrays but additional specific functionalities needed for deep learning\n",
    "- Brief exploration of PyTorch tensors, accessible from the torch module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create array filled with ones\n",
    "t_array = torch.ones((3,2))\n",
    "t_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_array = np.ones((3,2))\n",
    "n_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_array dtype: torch.float32\n",
      "n_array type: float64\n"
     ]
    }
   ],
   "source": [
    "# find the array type with *dtype*\n",
    "\n",
    "print(f't_array dtype: {t_array.dtype}')\n",
    "print(f'n_array type: {n_array.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 48, 224, 171,  44, 219, 240,  26,  15,  91, 111],\n",
       "        [150,  14, 115, 221,  63,  88,  82, 157,  71, 160],\n",
       "        [ 43, 249,  83, 197,   2,  67,   9, 209, 209, 178],\n",
       "        [ 39, 112, 228, 126,  53, 198, 211,  37, 186,  94],\n",
       "        [249,  44, 105, 102, 151, 235, 219, 165, 126, 210],\n",
       "        [103,  57, 213,   9, 133, 243, 115, 123,  55,  48],\n",
       "        [ 34, 207, 129, 125,  65,  18,   1,  62, 228, 106],\n",
       "        [178,  15, 176, 192,  86,  26, 222,   0, 249, 120],\n",
       "        [ 19, 120, 185, 126, 136, 183, 201,  32,   8, 188],\n",
       "        [115, 177, 241, 188, 139,  91, 139,  90,  21,  43]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch implements other funcs to create arrays similar to Numpy\n",
    "# eg. random number arrays:\n",
    "\n",
    "t_random = torch.randint(0, 255, (10,10))\n",
    "t_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from numpy to tensor\n",
    "t_from_n = torch.tensor(n_array)\n",
    "t_from_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy from tensor\n",
    "\n",
    "t_from_n.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(t_random);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing, broadcasting, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 48, 224, 171,  44, 219, 240,  26,  15,  91, 111],\n",
       "        [150,  14, 115, 221,  63,  88,  82, 157,  71, 160],\n",
       "        [ 43, 249,  83, 197,   2,  67,   9, 209, 209, 178],\n",
       "        [ 39, 112, 228, 126,  53, 198, 211,  37, 186,  94],\n",
       "        [249,  44, 105, 102, 151, 235, 219, 165, 126, 210],\n",
       "        [103,  57, 213,   9, 133, 243, 115, 123,  55,  48],\n",
       "        [ 34, 207, 129, 125,  65,  18,   1,  62, 228, 106],\n",
       "        [178,  15, 176, 192,  86,  26, 222,   0, 249, 120],\n",
       "        [ 19, 120, 185, 126, 136, 183, 201,  32,   8, 188],\n",
       "        [115, 177, 241, 188, 139,  91, 139,  90,  21,  43]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 48, 224, 171,  44, 219, 240,  26,  15,  91, 111])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_random[0,:] # row 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 42., 143., 183.,  36., 241.],\n",
       "        [ 42., 143., 183.,  36., 241.],\n",
       "        [ 42., 143., 183.,  36., 241.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# broadcasting allows the combining of tensors of different but compatible shapes:\n",
    "torch.ones((3,5)) * torch.randint(0, 255, (1,5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- may need to flatten arrays\n",
    "- eg. to create a fully connected layer in a deep learning network\n",
    "- done in two ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 48, 224, 171,  44, 219, 240,  26,  15,  91, 111, 150,  14, 115, 221,\n",
       "         63,  88,  82, 157,  71, 160,  43, 249,  83, 197,   2,  67,   9, 209,\n",
       "        209, 178,  39, 112, 228, 126,  53, 198, 211,  37, 186,  94, 249,  44,\n",
       "        105, 102, 151, 235, 219, 165, 126, 210, 103,  57, 213,   9, 133, 243,\n",
       "        115, 123,  55,  48,  34, 207, 129, 125,  65,  18,   1,  62, 228, 106,\n",
       "        178,  15, 176, 192,  86,  26, 222,   0, 249, 120,  19, 120, 185, 126,\n",
       "        136, 183, 201,  32,   8, 188, 115, 177, 241, 188, 139,  91, 139,  90,\n",
       "         21,  43])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_random.flatten()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify which contiguous dimensions you want to flatten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[74, 84, 52, 77],\n",
       "         [70, 55, 23, 46],\n",
       "         [ 2, 75, 18, 84]],\n",
       "\n",
       "        [[19, 53, 73, 61],\n",
       "         [63, 80, 30, 21],\n",
       "         [20, 97, 75, 43]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t_3d = torch.randint(0, 100, (2,3,4))\n",
    "t_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[74, 84, 52, 77, 70, 55, 23, 46,  2, 75, 18, 84],\n",
       "        [19, 53, 73, 61, 63, 80, 30, 21, 20, 97, 75, 43]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(t_3d, start_dim=1, end_dim=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative: \n",
    "\n",
    "- use the *view* method, which if possible, returns only a *view* of the array\n",
    "- can pass compatible dimensions to reshape the tensor, or simply use *-1* to completely flatten it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_random = torch.randint(0,255,(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 67,  16, 196, 253, 128,  45, 165,  25,  31,   7,   7,  91, 135, 215,\n",
       "         219, 138,  93,  72,  59,  40],\n",
       "        [ 71, 247, 224,   4,  17, 172, 173,  81, 234, 129,  22,  55, 161, 167,\n",
       "          64, 217, 117, 207, 126, 224],\n",
       "        [ 73, 150, 176,  86, 217,  12, 212,  23, 211,  54, 209, 120, 237, 152,\n",
       "          41, 192,  12,  40, 227, 169],\n",
       "        [ 47, 181,  67,  44, 142, 150,  41,  47, 143, 229, 240, 102, 222, 224,\n",
       "          77,  83, 123, 243,  43, 188],\n",
       "        [215, 155, 131, 152,  65,  11, 214, 154, 181,  14, 112,  58, 141, 214,\n",
       "         238, 168, 204, 248,  41,  72]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_random.view(5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 67,  16, 196, 253, 128,  45, 165,  25,  31,   7,   7,  91, 135, 215,\n",
       "        219, 138,  93,  72,  59,  40,  71, 247, 224,   4,  17, 172, 173,  81,\n",
       "        234, 129,  22,  55, 161, 167,  64, 217, 117, 207, 126, 224,  73, 150,\n",
       "        176,  86, 217,  12, 212,  23, 211,  54, 209, 120, 237, 152,  41, 192,\n",
       "         12,  40, 227, 169,  47, 181,  67,  44, 142, 150,  41,  47, 143, 229,\n",
       "        240, 102, 222, 224,  77,  83, 123, 243,  43, 188, 215, 155, 131, 152,\n",
       "         65,  11, 214, 154, 181,  14, 112,  58, 141, 214, 238, 168, 204, 248,\n",
       "         41,  72])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten\n",
    "t_random.view(-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are dealing with a *view*: If we modify one of the arrays *in place*, the values in the other arrays are changed as well. This means that this is **not** an independent array, but a shallow-copy. BE CAREFUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 67,  16, 196, 253, 128,  45, 165,  25,  31,   7,   7,  91, 135, 215,\n",
       "         219, 138,  93,  72,  59,  40],\n",
       "        [ 71, 247, 224,   4,  17, 172, 173,  81, 234, 129,  22,  55, 161, 167,\n",
       "          64, 217, 117, 207, 126, 224],\n",
       "        [ 73, 150, 176,  86, 217,  12, 212,  23, 211,  54, 209, 120, 237, 152,\n",
       "          41, 192,  12,  40, 227, 169],\n",
       "        [ 47, 181,  67,  44, 142, 150,  41,  47, 143, 229, 240, 102, 222, 224,\n",
       "          77,  83, 123, 243,  43, 188],\n",
       "        [215, 155, 131, 152,  65,  11, 214, 154, 181,  14, 112,  58, 141, 214,\n",
       "         238, 168, 204, 248,  41,  72]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_copy = t_random.view(5,20)\n",
    "view_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_copy.fill_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original affected\n",
    "t_random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to perform backpropogation in Deep Learning networks, we need to calculate all the necessary gradients.\n",
    "- this feature is integrated into PyTorch arrays directly:\n",
    "    - use the **require_grad** option\n",
    "    - simple example, define a variable **x = 1**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1, 1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass variable through a few simple operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y**(3/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 5 * z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last variable that depended initially on x, is now w. So w needs to be optimised in respect to variable x. We can do this simply by calculating the gradients of w **dw/dx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21.2132]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we indeed obtain the correct gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.213203435596427"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 * (3/2)*(2**0.5)*2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recover a numpy array from a PyTorch tensor or plot a PyTorch tensor with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x\u001b[39m.\u001b[39;49mnumpy()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.detach().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending tensors to a GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If pc has a compatible GPU, or if you run the notebook on Google Colab with a GPU runtime -> you can exploit Graphics card computing power"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will have to be \"pushed\" or \"pulled\" to and from that device. Can push entire networks but for the moment just send a tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check GPU is a available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLImaging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7be78ef86576dc85e4063c07c65a9db690a1cb6a564e4c0124da975355a61b8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
