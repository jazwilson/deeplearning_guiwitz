{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04: Simple neural net with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path containing data folder or use default for Colab (/gdrive/My Drive)\n",
    "local_folder = \"../data/\"\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/guiwitz/DLImaging/master/utils/check_colab.py\",\n",
    "    \"check_colab.py\",\n",
    ")\n",
    "from check_colab import set_datapath\n",
    "\n",
    "colab, datapath = set_datapath(local_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear layer that takes vector size 5 as input, and outputs a vector size 1044\n",
    "\n",
    "lin_layer = nn.Linear(in_features=5, out_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1783,  0.3561, -0.3590,  0.0736, -0.0596],\n",
       "         [ 0.3588, -0.4438, -0.1683,  0.0720,  0.1957],\n",
       "         [ 0.2292, -0.0698,  0.3378, -0.3111, -0.3138],\n",
       "         [ 0.3356, -0.0387, -0.3998,  0.3594,  0.4097],\n",
       "         [ 0.3697, -0.3972,  0.4299,  0.2861,  0.2349],\n",
       "         [ 0.3822,  0.0865,  0.3666, -0.2559,  0.1548],\n",
       "         [ 0.3819, -0.3540,  0.3386, -0.2987,  0.0684],\n",
       "         [-0.4457,  0.1852,  0.0741,  0.0278, -0.4095],\n",
       "         [-0.1144,  0.1446,  0.4327,  0.3990, -0.1856],\n",
       "         [-0.0266,  0.3403,  0.2048,  0.1113,  0.3524]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0945, -0.0139,  0.2263, -0.0307,  0.4064,  0.2341,  0.2265,  0.2531,\n",
       "         -0.1029,  0.1202], requires_grad=True)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the parameters in this single layer\n",
    "list(lin_layer.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing an input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our layer takes a vector as an input so let's try to creat a vector of size 5 and pass it through the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60, 71,  5,  7, 91])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myvector = np.random.randint(0, 100, 5)\n",
    "myvector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass it to lin_layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lin_layer(myvector)\n",
      "File \u001b[0;32m~/miniconda3/envs/env_dl/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/env_dl/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "lin_layer(myvector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an error, as the layer doesn't expect a Numpy array but a PyTorch tensor\n",
    "We can convert Numpy array into a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([60, 71,  5,  7, 91])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytensor = torch.tensor(myvector)\n",
    "mytensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lin_layer(mytensor)\n",
      "File \u001b[0;32m~/miniconda3/envs/env_dl/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/env_dl/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "lin_layer(mytensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error: The weights in the layer are by default float32. So the input should match this. However we passed 64 bit integers, creating a conflict. \n",
    "Modify the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([60., 71.,  5.,  7., 91.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytensor_float = mytensor.float()\n",
    "mytensor_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytensor_float.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do it earlier on while converting numpy to tensor:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytensor_float = torch.tensor(myvector, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 29.3733,   7.4734, -20.0223,  55.1558,  19.9163,  43.4370,   3.8295,\n",
       "        -50.0400,  -8.6345,  56.5557], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = lin_layer(mytensor_float)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output has expected size of 10!\n",
    "\n",
    "Can add other layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_layer2 = nn.Linear(10, 3)\n",
    "output = lin_layer2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to layers, we will also need activation functions such as soft max.\n",
    "\n",
    "These are implemented as modules in **torch.nn** as well as functions in **torch.functional** which we will use here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we use the functional form, we can pass the output of the above linear layer directly to the activation function, here as a ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_layer_activated = F.relu(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000, 20.9437,  0.0000], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_layer_activated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of a network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to create a layers and activation functions we can assemble them into a usable network structure. In PyTorch that structure is nn.Module a base class on top of which we can build our network. We can specify what parameters we want to pass when creating this object and we also have to define a single function, forward, which describes the network itself. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mynetwork(nn.Module):\n",
    "    def __init__(self, myparameter1, myparameter2):\n",
    "        super(Mynetwork, self).__init__()\n",
    "\n",
    "        # define layers here\n",
    "        self.layer1 = nn.Linear(myparameter1, 5)\n",
    "        self.layer2 = nn.Linear(5, myparameter2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # define the sequence of operations in the network including eg. activations\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.layer2(x)    \n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we defined: \n",
    "- a simple network defined by two parameters: **myparameter1**, **myparameter2** which is comprised of 2 linear layers and a ReLU unit.\n",
    "- The different layers that we need are defined in __init__ as object parameters and then re-used in the network definition in the **forward** function\n",
    "- **forward** takes an input **x** (eg. an image to classify), passes it through the network and outputs the result\n",
    "- So, in principal we could instantiate a model and use it like this: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        $ mymodel = Mynetwork(9,3)\n",
    "        $ mymodel.forward(myinput)\n",
    "\n",
    "However, **nn.Module** has a __call__ attribute that allows us to use the class as a function like this:\n",
    "\n",
    "        $ mymodel = Mynetwork(9,3)\n",
    "        $ mymodel(myinput)\n",
    "\n",
    "*This is how a model should be used properly, to expoit all the capabilities offered in PyTorch*\n",
    "\n",
    "Trying it out, we instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = Mynetwork(9, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like for the single linear layer before, we can have a look at all parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.3304, -0.0406, -0.2161, -0.2615, -0.0898, -0.1179,  0.1703, -0.0806,\n",
       "          -0.1688],\n",
       "         [-0.2263,  0.2773, -0.0357,  0.1517,  0.0399, -0.0994,  0.0964, -0.1243,\n",
       "          -0.1230],\n",
       "         [ 0.1338, -0.1805, -0.0814, -0.0399,  0.1184, -0.1443,  0.0473,  0.1608,\n",
       "          -0.0425],\n",
       "         [-0.2538,  0.1564,  0.2658,  0.1563, -0.0960,  0.1982,  0.0178,  0.0548,\n",
       "           0.1503],\n",
       "         [-0.3100, -0.2089, -0.3043, -0.1585,  0.0447, -0.1350,  0.2028,  0.0225,\n",
       "          -0.1704]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1032, -0.1842, -0.0524, -0.0418,  0.1983], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1828, -0.0054, -0.3419,  0.0007, -0.2423],\n",
       "         [-0.2979,  0.1850,  0.1159, -0.0752, -0.4303],\n",
       "         [-0.0548,  0.2920,  0.3381,  0.3030,  0.1242]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.3549,  0.3781,  0.4242], requires_grad=True)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mymodel.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Mynetwork(\n",
       "   (layer1): Linear(in_features=9, out_features=5, bias=True)\n",
       "   (layer2): Linear(in_features=5, out_features=3, bias=True)\n",
       " ),\n",
       " Linear(in_features=9, out_features=5, bias=True),\n",
       " Linear(in_features=5, out_features=3, bias=True)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mymodel.modules())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some repeats because we see here modules at all levels. E.g. each linear layer is a module but our entire network is a module as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also just find all modules contained in our main module and recover its name and function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: layer1 module: Linear(in_features=9, out_features=5, bias=True)\n",
      "name: layer2 module: Linear(in_features=5, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, module in mymodel.named_children():\n",
    "    print(f'name: {name} module: {module}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a dictionary of all layers with their weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[ 0.3304, -0.0406, -0.2161, -0.2615, -0.0898, -0.1179,  0.1703, -0.0806,\n",
       "                       -0.1688],\n",
       "                      [-0.2263,  0.2773, -0.0357,  0.1517,  0.0399, -0.0994,  0.0964, -0.1243,\n",
       "                       -0.1230],\n",
       "                      [ 0.1338, -0.1805, -0.0814, -0.0399,  0.1184, -0.1443,  0.0473,  0.1608,\n",
       "                       -0.0425],\n",
       "                      [-0.2538,  0.1564,  0.2658,  0.1563, -0.0960,  0.1982,  0.0178,  0.0548,\n",
       "                        0.1503],\n",
       "                      [-0.3100, -0.2089, -0.3043, -0.1585,  0.0447, -0.1350,  0.2028,  0.0225,\n",
       "                       -0.1704]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([ 0.1032, -0.1842, -0.0524, -0.0418,  0.1983])),\n",
       "             ('layer2.weight',\n",
       "              tensor([[ 0.1828, -0.0054, -0.3419,  0.0007, -0.2423],\n",
       "                      [-0.2979,  0.1850,  0.1159, -0.0752, -0.4303],\n",
       "                      [-0.0548,  0.2920,  0.3381,  0.3030,  0.1242]])),\n",
       "             ('layer2.bias', tensor([-0.3549,  0.3781,  0.4242]))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, pass an input through our network. It takes an input of size 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input = torch.randn((9,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6558, -2.4695,  1.3705, -0.8308, -0.5471, -0.0690, -0.2528, -0.9332,\n",
       "        -0.9735])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mymodel(my_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3975,  0.0962,  0.5062], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use batch processing:\n",
    "- we can pass batches of vectors through the network instead of single vectors\n",
    "- PyTorch layers are designed to handle this by default\n",
    "- eg. if we want to use a batch size = 32, we can use a 32 c 8 tensor \n",
    "- we can create one directly with **torch.randn** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 9])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tensor = torch.randn(32, 9)\n",
    "batch_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = mymodel(batch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing an image as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.draw import random_shapes\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 32x32 image of a circle.\n",
    "\n",
    "And invert image, set object as foreground:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_circle, _ = random_shapes((32,32),max_shapes=1, min_shapes=1, shape='circle',\n",
    "                                num_channels=1, channel_axis=None, min_size=8, random_seed=2)\n",
    "image_circle = 255-image_circle\n",
    "\n",
    "image_tensor = torch.tensor(image_circle, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_tensor, cmap='gray');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_flat = image_tensor.view(-1)\n",
    "image_flat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust network input size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = Mynetwork(1024, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mymodel(image_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2844, -0.7863, -0.0225, -0.4302,  0.4401, -1.0229,  1.0368, -0.0764,\n",
       "         0.4135, -1.4184, -0.4369, -1.5415, -0.9870, -1.2282,  0.7582, -0.4290,\n",
       "        -0.5496,  0.1577,  1.2395,  0.9378], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: flattening can be performed at different points. instead of flattening input, could have also flattened within our network definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mynetwork2(nn.Module):\n",
    "    def __init__(self, myparameter1, myparameter2):\n",
    "        super(Mynetwork2, self).__init__()\n",
    "\n",
    "        # define layers here\n",
    "        self.layer1 = nn.Linear(myparameter1, myparameter2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(-1) \n",
    "        # define the sequence of operations in the network including eg. activations\n",
    "        x = F.relu(self.layer1(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model = Mynetwork2(1024, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.9076,  0.0000,  0.0000,  0.0000,  8.9013,  0.0000,  0.3880,  4.8053,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 11.8445,  1.4290,  0.6757,\n",
       "         7.4704,  3.6068,  3.8881,  3.8326], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = second_model(image_tensor)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading a model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next notebook we will train our network. During this we will want to save our model and weights.\n",
    "\n",
    "\n",
    "Two ways to do this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save full model\n",
    "\n",
    "- Save the entire model so it can be reloaded\n",
    "- Can be problematic if moving saved models between computers.\n",
    "- Created a models folder in dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(second_model, datapath.joinpath('../models/simpleNN.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_model = torch.load(datapath.joinpath('../models/simpleNN.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.9076,  0.0000,  0.0000,  0.0000,  8.9013,  0.0000,  0.3880,  4.8053,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 11.8445,  1.4290,  0.6757,\n",
       "         7.4704,  3.6068,  3.8881,  3.8326], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_model(image_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can simply save all the parameters, which is a safer method. We simply recover them using state_dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(third_model.state_dict(),datapath.joinpath('../models/simpleNN_state.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reload the parameters, we first instantiate the model & and 'fill' with those values using **load_state_dict**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_model = Mynetwork2(1024, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[-1.0993e-02,  1.9869e-02,  2.3766e-02,  ..., -2.7438e-02,\n",
       "                        2.0099e-02,  1.7125e-02],\n",
       "                      [-8.0458e-03, -2.5301e-02, -3.0787e-02,  ...,  2.6594e-02,\n",
       "                        2.1697e-02,  1.2337e-02],\n",
       "                      [ 2.7537e-05,  2.1004e-02, -2.4900e-02,  ...,  1.4359e-02,\n",
       "                        2.8169e-02,  1.0439e-03],\n",
       "                      ...,\n",
       "                      [ 1.3788e-02, -1.8615e-02, -2.4981e-02,  ...,  1.3405e-02,\n",
       "                       -2.3429e-02, -2.1308e-02],\n",
       "                      [-2.8702e-02, -2.0998e-02,  6.0449e-03,  ..., -2.6826e-02,\n",
       "                        1.1193e-02,  1.8285e-02],\n",
       "                      [-2.4406e-02,  7.9038e-03,  1.2152e-02,  ...,  5.2912e-03,\n",
       "                        2.4046e-02, -1.4188e-02]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([-0.0218, -0.0026,  0.0074,  0.0002, -0.0273,  0.0270, -0.0117,  0.0107,\n",
       "                      -0.0050, -0.0088,  0.0078,  0.0024, -0.0092, -0.0008,  0.0274, -0.0124,\n",
       "                      -0.0193, -0.0022,  0.0199,  0.0025]))])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.load(datapath.joinpath('../models/simpleNN_state.pt'))\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourth_model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.9076,  0.0000,  0.0000,  0.0000,  8.9013,  0.0000,  0.3880,  4.8053,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 11.8445,  1.4290,  0.6757,\n",
       "         7.4704,  3.6068,  3.8881,  3.8326], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourth_model(image_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Running on a GPU (run on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "Running on a GPU (run on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = torch.device(\"cuda\")\n",
    "else:\n",
    "    dev = torch.device(\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
